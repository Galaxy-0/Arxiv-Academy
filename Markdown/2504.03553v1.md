
# 具身知识性自我意识

乔硕斐♠∗，邱志松♠∗，任柏昌♠，王晓斌♢，茹祥源♠，  
张宁豫♠†，陈翔♣，姜勇♢†，谢鹏军♢，黄非♢，陈华钧♠♡†  
♠浙江大学  
♢阿里巴巴集团  
♣南京航空航天大学  
♡浙江省大数据智能计算重点实验室  
{shuofei,zhangningyu,huajunsir}@zju.edu.cn  

## 摘要

大语言模型（LLMs）在各种具身规划任务中已取得显著性能。然而，传统智能体规划方法采用"大水漫灌"模式，将黄金轨迹、外部反馈和领域知识不加区分地注入智能体模型。这种做法忽视了人类决策过程中情境化自我意识的基本认知原则——即动态评估情境需求并策略性调配决策资源的能力。

我们提出具身知识性自我意识来解决这一缺陷，这是一种使基于LLM的智能体能够自主调节知识使用的新范式。具体而言，我们提出KnowSelf——一种数据驱动的方法，赋予智能体类人的知识性自我意识。我们设计了一种启发式情境判断标准，通过在智能体自主探索的轨迹上标记特殊符号来收集训练数据。通过两阶段训练过程，智能体模型可以通过生成特定符号在不同情境间切换，以最小成本实现最优规划效果。实验表明，KnowSelf在不同任务和模型上均能超越各类强基线，且仅需极少量外部知识[^1]。

[^1]: 代码已开源：https://github.com/zjunlp/KnowSelf

## 1 引言

大语言模型的显著进步推动了基于智能体的规划系统发展（Xi等，2023；Wang等，2024a；Huang等，2024；Durante等，2024；Liu等，2025）。根据智能体学习决策的方式，现有方法可分为三类：  
i) 直接轨迹模仿（Yao等，2023；Chen等，2023；Zeng等，2023）；  
ii) 试错优化（Shinn等，2023；Xiang等，2023；Song等，2024b；Zhang等，2024a）；  
iii) 知识增强规划（Zhao等，2024a；Fu等，2024；Zhu等，2024；Chen等，2024）。

然而，现有智能体学习更类似于无意识的模式拟合过程（Mirzadeh等，2024；Shi等，2023；Dziri等，2023）。智能体模型通过被动接收显式规划轨迹来学习隐式规划能力，导致推理过程对意外信号脆弱，易陷入模式崩溃。引入外部反馈或知识的增强方法往往采用"大水漫灌"策略，忽视智能体的真实需求。过度试错和盲目知识融合不仅实际不可行，还会显著增加模型推理成本。

反观人类决策，自我意识是核心要素（Keenan等，2011；Lewis等，2011；Lou等，2017）。它使个体能够评估认知状态并动态调整策略。这种元认知能力帮助人类判断何时依靠自身能力、何时需要反思、何时需要外部知识，从而优化决策过程。相比之下，现有语言智能体缺乏这种自我意识能力，导致低效脆弱的规划行为。那么，语言智能体能否像人类一样具有情境化自我意识？

本文提出具身知识性自我意识问题，即智能体对当前环境情境下是否具备提供正确行动能力的认知。为此，我们提出KnowSelf——一种数据驱动方法，赋予智能体模型知识性自我意识能力，使其能根据环境情境选择性引入知识（见图1）。

![图1: 具身知识性自我意识](https://via.placeholder.com/400x200?text=Figure+1:+Agentic+Knowledgeable+Self-awareness)

## 2 背景

动态交互环境可建模为部分可观测马尔可夫决策过程：$(U, S, A, T , O)$。初始任务$u \in U$通常伴随初始环境状态$s_0 \in S$。给定当前状态$s$，执行动作$a \in A$后，状态转移函数$T(s'|s, a) \in T$决定下一状态$s'$。由于部分可观测，当前状态以观测$o \in O$形式提供给语言智能体。时间步$t$的历史交互轨迹可表示为$h_t = (u, a_0, o_0, a_1, o_1, \ldots, a_t, o_t)$。本场景中，由参数$\theta$的LLM支持的智能体$\pi$负责根据$h_t$决定下一动作：

$$a_{t+1} \sim \pi_\theta(\cdot|h_t).$$

现有方法多依赖公式1进行决策，这更类似于机械记忆。因此，本文提出具身知识性自我意识。需注意，此处的自我意识不同于LLM知识边界的概念（Cheng等，2024；Yin等，2024；Wen等，2024），其关注动态情境中的自我意识而非静态事实知识。具体定义三种情境类型：

- **快思考**：智能体无需深入思考即可直接给出正确行动
- **慢思考**：智能体需多步思考反思才能给出正确行动
- **知识思考**：智能体无法独立给出正确行动，需依赖外部知识

我们超越快/慢思考范式（Yu等，2024；Saha等，2024；Chen等，2025；Li等，2025），将外部知识引入LLM思考系统，着力增强语言智能体的知识性自我意识能力。

## 3 方法

### 3.1 知识系统构建

由于重点在知识性自我意识而非知识系统构建，我们借鉴Chen等（2024）提出的轻量级知识收集方法。知识库构建是离线的，仅需极少量轨迹即可完成。完整构建过程见附录A。最终知识系统表示为$S : (K, R)$，其中$K = \{k_1, k_2, ..., k_{N_{max}}\}$为知识库，$R$为基于历史轨迹$h_t$的知识选择模块。需注意，本文"知识"涵盖符号知识、参数化知识、网络搜索知识等形式。不同检索器的对比实验见附录I。

### 3.2 情境判断标准

基于公式1和第2节定义，我们将智能体情境分为三类。给定历史$h_t$，标准下一动作为$a_{t+1}$，智能体直接预测动作为$a^p_{t+1}$。当预测错误时允许反思，得到修正动作$a^r_{t+1} = \text{rethink}(h_t, a^p_{t+1})$。判断标准$C$如下：

1. **快思考**：$a^p_{t+1} = a_{t+1}$，智能体直接生成正确动作
2. **慢思考**：$a^p_{t+1} \neq a_{t+1}, a^r_{t+1} = a_{t+1}$，需反思后生成正确动作
3. **知识思考**：$a^p_{t+1}, a^r_{t+1} \neq a_{t+1}$，需依赖外部知识

该标准指导构建情境感知数据，使智能体自主判断情境。选择性机制可大幅降低过度反思和知识使用的训练/推理成本。

### 3.3 自我意识应用

我们设计数据驱动方法KnowSelf赋予智能体知识性自我意识能力（见图2）。

![图2: KnowSelf框架](https://via.placeholder.com/600x400?text=Figure+2:+The+framework+of+our+KnowSelf)

#### 数据构建

给定历史-动作对$(h_t, a_{t+1})$和未训练智能体$\pi_\theta$，基于情境标准$C$构建监督式自我意识数据。若智能体直接生成正确动作（快思考），直接使用$y = a_{t+1}$作为输出。若首轮生成错误动作$a^p_{t+1}$，则给予反思提示[^2]。反思过程的思维链记为$ret$。若反思后动作正确（慢思考），则输出：

$$y = [a^p_{t+1}, \text{Reflection} <r>ret</r>, a_{t+1}],$$

其中$[]$表示用`\n`连接，`Reflection`为慢思考情境标记，`<r>`和`</r>`为思维链边界符。若反思后仍错误（知识思考），则通过选择模型$R$从$K$选取知识$know$，输出：

$$y = [\text{Knowledge} <k>know</k>, a_{t+1}],$$

其中`Knowledge`为知识思考标记，`<k>`和`</k>`为知识边界符。遍历所有输入-输出对后，得到自我意识训练数据$D_{self}$。

[^2]: 反思提示详见附录J.3
[^3]: 知识选择细节见附录B

#### 自我意识学习

采用两阶段训练流程。首阶段通过自回归损失训练参考智能体$\pi_{ref}$：

$$L_{SFT} = -\mathbb{E}_{(h_t,y)\sim D_{self}} \log \pi_\theta(y|h_t).$$

随后让参考智能体在$D_{self}$上探索，收集错误预测$y^p$作为负样本构建配对意识数据集$D_{pair}$。第二阶段引入离线DPO目标：

$$L_{DPO} = -\mathbb{E}_{(h_t,y,y^p)\sim D_{pair}} \left[ \log \sigma \left( \beta \log \frac{\pi_\theta(y|h_t)}{\pi_{ref}(y|h_t)} - \beta \log \frac{\pi_\theta(y^p|h_t)}{\pi_{ref}(y^p|h_t)} \right) \right].$$

由于正确动作空间狭窄，参考Pang等（2024）方法，在第二阶段重新引入经输出长度归一化的SFT损失：

$$L_{NLL} = -\mathbb{E}_{(h_t,y,y^p)\sim D_{pair}} \frac{\log \pi_\theta(y|h_t)}{|y|},$$

得到最终损失：

$$L_{RPO} = L_{DPO} + \alpha L_{NLL},$$

其中$\alpha$为平衡超参。训练时扩展模型词表以适配新增特殊符号。不同训练策略的影响分析见附录H。

#### 自我意识推理

推理过程中，若智能体首轮后停止输出，则直接将预测动作存入历史$h_t$。若生成`Reflection`标记，则允许继续反思过程并将修正动作存入历史。若直接生成`Knowledge`标记，则通过$R$从$K$选择知识，将其加入上下文后生成最终动作。运行示例见图2。

## 4 实验

### 4.1 实验设置

#### 数据集与指标

我们在ALFWorld（Shridhar等，2021）和WebShop（Yao等，2022）两个真实世界模拟规划数据集上评估KnowSelf。ALFWorld是家庭环境数据集，奖励为0/1二元指标。WebShop是在线购物环境，提供0-1连续奖励衡量任务完成度。因此我们采用平均奖励作为最终指标。黄金训练轨迹来自AgentBank（Song等，2024a）。数据集细节见附录E。

#### 模型与基线

评估模型包括：  
1) Gemma-2B（Rivière等，2024），gemma-2-2b-it版本  
2) Llama-8B（Dubey等，2024），Llama-3.1-8B-Instruct版本  

对比基线包括：  
- 通用规划方法：REACT（Yao等，2023）  
- 试错优化方法：Reflexion（Shinn等，2023）和ETO（Song等，2024b）  
- 知识增强方法：ExpeL（Zhao等，2024a）、KnowAgent（Zhu等，2024）、WKM（Qiao等，2024b）  

另以GPT-4o（gpt-4o-2024-08-06）（Hurst等，2024）作为强上界基线。定义Know%表示知识增强动作占比。所有提示基线均采用双样本测试。更多基线及复现细节见附录F。

#### 训练与推理

首阶段学习率2e-5，批次大小8；第二阶段学习率5e-7，批次大小3。DPO损失中$\beta=0.5$，平衡因子$\alpha=1$。首阶段训练3轮，第二阶段1轮。推理时温度固定为0。实验在8块NVIDIA A800 80G GPU上完成。细节见附录G。

### 4.2 主要结果

#### 无知识基线对比

表2&3显示KnowSelf在无知识基线（Know%=0%）上的对比结果。在Llama-8B和Gemma-2B上均持续优于无知识基线。Gemma-2B性能甚至超越GPT-4o的REACT。Llama-8B与GPT-4o的Reflexion（允许最多5次尝试）性能相当，凸显知识在规划中的重要性。

#### 有知识基线对比

与知识增强基线的对比显示（表2&3），KnowSelf以极低知识量超越全知识基线。这表明并非所有知识都有效。同时发现Gemma-2B在ExpeL上的表现甚至差于REACT，结合消融实验（图3a）可知，过度知识增强可能损害弱模型性能。

值得注意的是，KnowSelf在Llama-8B上仅需15.01%（ALFWorld）和17.12%（WebShop）的知识率即超越GPT-4o的ExpeL。且Llama-8B在ALFWorld上以更低知识率（15.01%）获得优于Gemma-2B的表现。