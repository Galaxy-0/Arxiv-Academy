Here's the precise Markdown conversion optimized for Obsidian:

# Agentic Knowledgeable Self-awareness

Shuofei Qiao♠∗, Zhisong Qiu♠∗, Baochang Ren♠, Xiaobin Wang♢, Xiangyuan Ru♠,  
Ningyu Zhang♠†, Xiang Chen♣, Yong Jiang♢†, Pengjun Xie♢, Fei Huang♢, Huajun Chen♠♡†  
♠Zhejiang University  
♢Alibaba Group  
♣Nanjing University of Aeronautics and Astronautics  
♡Zhejiang Key Laboratory of Big Data Intelligent Computing  
{shuofei,zhangningyu,huajunsir}@zju.edu.cn  

## Abstract

Large Language Models (LLMs) have achieved considerable performance across various agentic planning tasks. However, traditional agent planning approaches adopt a "flood irrigation" methodology that indiscriminately injects gold trajectories, external feedback, and domain knowledge into agent models. This practice overlooks the fundamental human cognitive principle of situational self-awareness during decision-making—the ability to dynamically assess situational demands and strategically employ resources during decision-making.

We propose agentic knowledgeable self-awareness to address this gap, a novel paradigm enabling LLM-based agents to autonomously regulate knowledge utilization. Specifically, we propose KnowSelf, a data-centric approach that applies agents with knowledgeable self-awareness like humans. Concretely, we devise a heuristic situation judgement criterion to mark special tokens on the agent's self-explored trajectories for collecting training data. Through a two-stage training process, the agent model can switch between different situations by generating specific special tokens, achieving optimal planning effects with minimal costs. Our experiments demonstrate that KnowSelf can outperform various strong baselines on different tasks and models with minimal use of external knowledge$ ^1 $.

$ ^1 $Code is available at https://github.com/zjunlp/KnowSelf.

## 1 Introduction

Remarkable advances in Large Language Models (LLMs) have catalyzed breakthroughs in agent-based planning systems (Xi et al., 2023; Wang et al., 2024a; Huang et al., 2024; Durante et al., 2024; Liu et al., 2025). According to how agents learn decision-making, current agent learning methods can be categorized into three types:  
i) direct trajectory imitation (Yao et al., 2023; Chen et al., 2023; Zeng et al., 2023);  
ii) trial-and-error refinement (Shinn et al., 2023; Xiang et al., 2023; Song et al., 2024b; Zhang et al., 2024a);  
iii) knowledge-augmented planning (Zhao et al., 2024a; Fu et al., 2024; Zhu et al., 2024; Chen et al., 2024).

However, current agent learning resembles more of an unconscious pattern-fitting process (Mirzadeh et al., 2024; Shi et al., 2023; Dziri et al., 2023). Agent models are compelled to learn implicit planning capabilities by being indiscriminately fed explicit planning trajectories, leading to a fragility towards unexpected signals during the inference process, thereby easily dropping into pattern collapse. Further enhanced approaches such as the introduction of external feedback or knowledge often tend to be a "flood irrigation" strategy, disregarding the agents' real necessity. However, excessive trial-and-error and blind incorporation of knowledge are usually unfeasible in practical settings and markedly elevate the inference cost of the model.

Conversely, self-awareness is a critical component of human decision-making (Keenan et al., 2011; Lewis et al., 2011; Lou et al., 2017). It allows individuals to assess their cognitive states and adapt their strategies according to dynamic external situations. This metacognitive ability enables humans to recognize when they can rely on their own abilities, when they need self-reflection, or when they need additional knowledge, thus optimizing their decision-making processes. On the contrary, current language agents lack this self-awareness capability, often leading to inefficient and brittle planning behaviors. So can language agents also have situational self-awareness like humans?

In this paper, we introduce the problem of agentic knowledgeable self-awareness which refers to the agent's cognition of whether itself has the ability to provide the correct next action given the current environmental situation. To tackle this problem, we propose KnowSelf, a data-driven method that endows agent models with the ability of knowledgeable self-awareness which enables agent models to selectively introduce knowledge based on the current situation in the environment (see Figure 1).

## 2 Background

A dynamic interactive environment can be regarded as a Partially Observable Markov Decision Process: $ (U, S, A, T , O) $. Initially, a specific task $ u \in U $ is typically accompanied by an initial environmental state $ s\_0 \in S $. Given the current state $ s $, after performing an action $ a \in A $, the state transition function $ T(s'|s, a) \in T $ determines the next state $ s' $. Due to partial observation, the current state is provided to the language agent in the form of an observation
$ o \in O $
. Then the historical interaction trajectory at time $ t $ can be represented as $ h\_t = (u, a\_0, o\_0, a\_1, o\_1, \ldots, a\_t, o\_t) $. In our scenario, a language agent $ \pi $ backed by an LLM with parameters $ \theta $ is responsible for deciding the next action $ a_{t+1} $ based on the historical trajectory $ h\_t $:

$$ a_{t+1} \sim \pi_\theta(\cdot|h\_t). $$ Most current methods rely on fitting Equation 1 to make decisions, which is more akin to rote memorization. So in this paper, we propose agentic knowledgeable self-awareness. Please note that the self-awareness mentioned here differs from the previous concept of LLMs' knowledge boundary (Cheng et al., 2024; Yin et al., 2024; Wen et al., 2024). The focus here is on the agent's self-awareness in dynamic situations, rather than on static factual knowledge. Specifically, we define three types of situations based on agents' ability:

- **Fast thinking**: The agent is able to directly provide the correct action with little thinking.
- **Slow thinking**: The agent is able to provide the correct action but requires multiple steps of thinking and reflection.
- **Knowledgeable thinking**: The agent is unable to provide the correct action and needs to rely on external knowledge for thinking.

## 3 Method

### 3.1 Knowledge System Construction

Given that our emphasis is on knowledgeable self-awareness rather than the construction of a knowledge system, we draw upon and polish up a simple yet effective knowledge collection method outlined in Chen et al. (2024) to minimize costs in this process. The formation of the knowledge base is offline and lightweight, relying on an extremely minimal number of trajectories to be completed. A detailed knowledge system construction process can be found in Appendix A. We denote the final knowledge system as $ S : (K, R) $, where $ K = \{k\_1, k\_2, \ldots, k_{N_{max}}\} $ is the knowledge base and $ R $ is the knowledge selection module that can select the required knowledge based on the agent's historical trajectory $ h\_t $.

### 3.2 Situation Judgement Criterion

Based on Equation 1 and our definition of three situations in 2, we classify the agent's situations into three types. Assuming the given history is denoted as $ h\_t $, the gold next action is described as $ a_{t+1} $, and the next action predicted directly by the agent is represented as $ a^p_{t+1} $. We allow the agent to rethink when the predicted action is incorrect, resulting in a revised action denoted as $ a^r_{t+1} = \text{rethink}(h\_t, a^p_{t+1}) $. We then determine the agent's situation according to the following criteria $ C $:  
i) **Fast Thinking**: $ a^p_{t+1} = a_{t+1} $. The agent can directly generate the correct action.  
ii) **Slow Thinking**: $ a^p_{t+1} \neq a_{t+1}, a^r_{t+1} = a_{t+1} $. The agent can generate the correct action but needs rethinking.  
iii) **Knowledgeable Thinking**: $ a^p_{t+1}, a^r_{t+1} \neq a_{t+1} $. The agent is unable to generate the correct action, so it needs knowledge.

### 3.3 Self-awareness Apply

We design a data-driven method called KnowSelf to endow the agent with agentic knowledgeable self-awareness capabilities as shown in Figure 2.

#### Data Construction

Given the history-action pair $ (h\_t, a_{t+1}) $ and an untrained agent $ \pi_\theta $, we augment the original action based on the situation criterion $ C $ to construct the supervised self-awareness data. If the agent determines a correct action $ a^p_{t+1} $ (Fast Thinking), $ y = a_{t+1} $ will be directly used as the output. If the agent provides an incorrect action $ a^p_{t+1} $ in the first trial, it will be given a prompt to rethink$ ^2 $. The chain of thought during this rethinking process is denoted as $ ret $. If the determined action $ a^r_{t+1} $ after rethinking is correct (Slow Thinking), the output at this point is:

$$ y = [a^p_{t+1}, \text{Reflection} <r>ret</r>, a_{t+1}], $$ where $ [] $ represents concat with $ \backslash n $, Reflection is a special token used to mark the situation of Slow Thinking, $ <r> $ and $ </r> $ are special tokens surrounding the $ ret $. However, if the reflected action $ a^r_{t+1} $ is incorrect, we introduce knowledge (Knowledgeable Thinking). We use the selection model $ R $ to choose the most appropriate piece of knowledge$ ^3 $ $ know $ from the knowledge base $ K $ and then the output at this situation is:

$$ y = [ \text{Knowledge} <k>know</k>, a_{t+1}], $$ where Knowledge is the situational special token, $ <k> $ and $ </k> $ are special tokens surrounding the knowledge. After traversing all input-output pairs, we obtain the self-awareness training data $ D_{self} $.

#### Self-awareness Learning

We apply a two-stage training process to teach the naive agent on our curated agentic knowledgeable awareness dataset $ D_{self} $. First, we train with the autoregressive loss to obtain the reference agent $ \pi_{ref} $:

$$ L_{SFT} = -\mathbb{E}_{(h\_t,y)\sim D_{self}} \log \pi_\theta(y|h\_t). $$ Then we enable the reference agent to explore on $ D_{self} $ and collect the predicted $ y^p $ with wrong actions as negative samples to construct a pair-wise awareness dataset $ D_{pair} $. In the second stage, we additionally introduce an offline DPO objective to further boost the self-awareness performance:

$$ L_{DPO} = -\mathbb{E}_{(h\_t,y,y^p)\sim D_{pair}} \left[ \log \sigma \left( \beta \log \frac{\pi_\theta(y|h\_t)}{\pi_{ref}(y|h\_t)} - \beta \log \frac{\pi_\theta(y^p|h\_t)}{\pi_{ref}(y^p|h\_t)} \right) \right]. $$ Due to the narrow space of correct actions, following Pang et al. (2024), we re-introduce the SFT loss and normalize it by the output length in the second stage to stabilize the training process: $$ L_{NLL} = -\mathbb{E}_{(h\_t,y,y^p)\sim D_{pair}} \frac{\log \pi_\theta(y|h\_t)}{|y|}, $$ resulting in the final loss for this stage: $$ L_{RPO} = L_{DPO} + \alpha L_{NLL}, $$ where $ \alpha $ is a hyperparameter to balance the two loss terms.

#### Self-awareness Inference

During the inference process, if the agent stops outputting after the first trial, we directly place the predicted action in the history $ h\_t $ for the next-step decision. If the agent generates Reflection after the first action, we allow it to continue the reflective process and place the reflected action into $ h\_t $. If the agent directly generates Knowledge, we use $ R $ to choose a piece of knowledge from $ K $. We append the selected knowledge to the context to allow the agent to continue this step, and then place the generated action into the history for the next decision.

## 4 Experiments

### 4.1 Experimental Settings

#### Datasets and Metrics

We evaluate KnowSelf on two real-world simulated planning datasets: ALFWorld (Shridhar et al., 2021) and WebShop (Yao et al., 2022). ALFWorld is a household dataset requiring the agent to navigate through the room and manipulate objects. The reward of ALFWorld is binary 0 or 1, indicating whether the agent has completed the task or not. WebShop is an online shopping dataset in a website environment. It provides dense final rewards from 0 to 1 to measure the completion level of the task. So for all the datasets, we apply Average Reward as the final metrics.

#### Models and Baselines

We evaluate KnowSelf on two open-source models with different scales:  
1) Gemma-2B (Rivière et al., 2024), the gemma-2-2b-it version;  
2) Llama-8B (Dubey et al., 2024), the Llama-3.1-8B-Instruct version.  

To demonstrate validity, we compare KnowSelf with:  
- One general agent planning method: REACT (Yao et al., 2023)  
- Two agent planning methods with trial-and-error: Reflexion (Shinn et al., 2023) and ETO (Song et al., 2024b)  
- Three knowledge-augmented methods: ExpeL (Zhao et al., 2024a), KnowAgent (Zhu et al., 2024), WKM (Qiao et al., 2024b)  

We also include GPT-4o (gpt-4o-2024-08-06) (Hurst et al., 2024) as a strong upper-bound baseline.

### 4.2 Main Results

#### Comparison with baselines w/o knowledge

Table 2&3 show the comparison between our method and baselines without knowledge (Know%=0%). KnowSelf consistently demonstrates superiority over baselines without knowledge on both Llama-8B and Gemma-2B. The performance of Gemma-2B even surpasses that of GPT-4o's REACT. Furthermore, our Llama-8B model performs comparably to GPT-4o's Reflexion.

#### Comparison with baselines w/ knowledge

From Table 2&3, it can be observed that KnowSelf surpasses all 100% knowledge baselines with a minimal amount of knowledge. This clearly demonstrates that not all knowledge is effective during agent planning. Notably, our KnowSelf, with only 15.01% and 17.12% knowledge rate on Llama-8B, surpasses GPT-4o's ExpeL on ALFWorld and WebShop.

## 5 Analysis

### Knowledgeable self-awareness is beneficial to break planning pattern overfitting

Figure 3a illustrates the impact on the performance of KnowSelf when certain key steps are replaced. w/o ret denotes the exclusion of reflection. w/o know signifies only using the model's reflective capabilities. w/o all represents the retention of only fast thinking. We also introduce knowledge at every step to create a scenario with know%=100% (w/ full know). It can be observed that training directly on gold trajectories (w/o all) is more akin to fitting patterns in trajectories while introducing reflective and knowledgable self-awareness can enable agents to plan better.

### KnowSelf can better elicit the generalization of agent planning

We select three simple tasks (i.e. Put, Clean, Examine) on ALFWorld as the training set and evaluate the generalization ability of KnowSelf on three other challenging tasks (i.e. Heat, Cool, PutTwo). Figure 3b illustrates the OOD performance of KnowSelf compared to baselines. We observe that whether to introduce external knowledge, the trained baselines exhibit serious overfitting.

### The performance of KnowSelf advances with the increase of the model scales and the training data volumes

In Figure 3c, we explore the scaling law of self-awareness from two perspectives: model size and volume of self-awareness training data. Regarding data volume, we analyze it from both relative and absolute standpoints. Overall, in various settings, the performance of Llama-8B is superior to Gemma-2B.

### Knowledgeable self-awareness emerges in the last few layers of agent models

To understand the mechanism of agentic knowledgeable self-awareness, we collect data on both fast thinking and knowledgeable thinking from ALFWorld to investigate how models make decisions on whether to invoke knowledge in the context of next token prediction. We calculate the average probabilities of tokens representing various situations in each layer of the Transformer on all data, as illustrated in Figure 4. It can be observed that due to the absence of slow thinking, the probability of the Reflection token remains consistently at 0.

## References

[All references would be listed here in proper citation format]

*Note: The complete paper would include all sections, figures, tables, and appendices with proper mathematical formatting and Markdown syntax as shown above.*
