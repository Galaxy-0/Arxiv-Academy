# Agentic Knowledgeable Self-awareness Shuofei Qiao♠∗, Zhisong Qiu♠∗, Baochang Ren♠, Xiaobin Wang♢, Xiangyuan Ru♠, Ningyu Zhang♠†, Xiang Chen♣, Yong Jiang♢†, Pengjun Xie♢, Fei Huang♢, Huajun Chen♠♡† ♠Zhejiang University ♢Alibaba Group ♣Nanjing University of Aeronautics and Astronautics ♡Zhejiang Key Laboratory of Big Data Intelligent Computing {shuofei,zhangningyu,huajunsir}@zju.edu.cn ## Abstract Large Language Models (LLMs) have achieved considerable performance across various agentic planning tasks. However, traditional agent planning approaches adopt a "flood irrigation" methodology that indiscriminately injects gold trajectories, external feedback, and domain knowledge into agent models. This practice overlooks the fundamental human cognitive principle of situational self-awareness during decision-making—the ability to dynamically assess situational demands and strategically employ resources during decision-making. We propose agentic knowledgeable self-awareness to address this gap, a novel paradigm enabling LLM-based agents to autonomously regulate knowledge utilization. Specifically, we propose KnowSelf, a data-centric approach that applies agents with knowledgeable self-awareness like humans. Concretely, we devise a heuristic situation judgement criterion to mark special tokens on the agent's self-explored trajectories for collecting training data. Through a two-stage training process, the agent model can switch between different situations by generating specific special tokens, achieving optimal planning effects with minimal costs. Our experiments demonstrate that KnowSelf can outperform various strong baselines on different tasks and models with minimal use of external knowledge $^1$ . $^1$ Code is available at https://github.com/zjunlp/KnowSelf. ## 1 Introduction Remarkable advances in Large Language Models (LLMs) have catalyzed breakthroughs in agent-based planning systems (Xi et al., 2023; Wang et al., 2024a; Huang et al., 2024; Durante et al., 2024; Liu et al., 2025). According to how agents learn decision-making, current agent learning methods can be categorized into three types: i) direct trajectory imitation (Yao et al., 2023; Chen et al., 2023; Zeng et al., 2023); ii) trial-and-error refinement (Shinn et al., 2023; Xiang et al., 2023; Song et al., 2024b; Zhang et al., 2024a); iii) knowledge-augmented planning (Zhao et al., 2024a; Fu et al., 2024; Zhu et al., 2024; Chen et al., 2024). However, current agent learning resembles more of an unconscious pattern-fitting process (Mirzadeh et al., 2024; Shi et al., 2023; Dziri et al., 2023). Agent models are compelled to learn implicit planning capabilities by being indiscriminately fed explicit planning trajectories, leading to a fragility towards unexpected signals during the inference process, thereby easily dropping into pattern collapse. Further enhanced approaches such as the introduction of external feedback or knowledge often tend to be a "flood irrigation" strategy, disregarding the agents' real necessity. However, excessive trial-and-error and blind incorporation of knowledge are usually unfeasible in practical settings and markedly elevate the inference cost of the model. Conversely, self-awareness is a critical component of human decision-making (Keenan et al., 2011; Lewis et al., 2011; Lou et al., 2017). It allows individuals to assess their cognitive states and adapt their strategies according to dynamic external situations. This metacognitive ability enables humans to recognize when they can rely on their own abilities, when they need self-reflection, or when they need additional knowledge, thus optimizing their decision-making processes. On the contrary, current language agents lack this self-awareness capability, often leading to inefficient and brittle planning behaviors. So can language agents also have situational self-awareness like humans? In this paper, we introduce the problem of agentic knowledgeable self-awareness which refers to the agent's cognition of whether itself has the ability to provide the correct next action given the current environmental situation. To tackle this problem, we propose KnowSelf, a data-driven method that endows agent models with the ability of knowledgeable self-awareness which enables agent models to selectively introduce knowledge based on the current situation in the environment (see Figure 1). Specifically, we enable the agent to self-explore and gather different situations within the environment. Then we design a heuristic criterion to classify three kinds of situations (fast thinking, slow thinking, knowledgeable thinking) and mark them with special tokens to produce self-awareness training data. Subsequently, a two-stage training process is employed to train the agent model's self-awareness capability. We first conduct supervised fine-tuning to teach agent models initial self-awareness planning patterns. Then we utilize an RPO loss (Pang et al., 2024) to further boost self-awareness abilities. Finally, the agent signifies its situational awareness by generating certain special tokens, enabling selective reflection or knowledge incorporation during inference. We evaluate KnowSelf on two simulated agent planning datasets with two different scales of models. Experimental results show that KnowSelf can achieve superior performance with minimal reflection and knowledge compared to various baselines. Moreover, we conduct further analysis to examine the scaling law, generalization, and mechanism of agentic knowledgeable self-awareness. In a nutshell, our contributions are as follows: - **Problem and Method**: We propose agentic knowledgeable self-awareness and introduce KnowSelf to enable agent models to selectively query knowledge based on situations. - **Experiments**: Experimental results show that KnowSelf can achieve optimal performance with minimal reflection and knowledge compared to various baselines. - **Analysis**: Except for ablation studies, we further explore the scaling law, generalization, and mechanism of agentic self-awareness. ## 2 Background A dynamic interactive environment can be regarded as a Partially Observable Markov Decision Process: $(U, S, A, T , O)$ . Initially, a specific task $u \in U$ is typically accompanied by an initial environmental state $s_0 \in S$ . Given the current state $s$ , after performing an action $a \in A$ , the state transition function $T(s'|s, a) \in T$ determines the next state $s'$ . Due to partial observation, the current state is provided to the language agent in the form of an observation $o \in O$ . Then the historical interaction trajectory at time $t$ can be represented as $h_t = (u, a_0, o_0, a_1, o_1, \ldots, a_t, o_t)$ . In our scenario, a language agent $\pi$ backed by an LLM with parameters $\theta$ is responsible for deciding the next action $a_{t+1}$ based on the historical trajectory $h_t$ : $$ a_{t+1} \sim \pi_\theta(\cdot|h_t). $$ Most current methods rely on fitting Equation 1 to make decisions, which is more akin to rote memorization. So in this paper, we propose agentic knowledgeable self-awareness. Please note that the self-awareness mentioned here differs from the previous concept of LLMs' knowledge boundary (Cheng et al., 2024; Yin et al., 2024; Wen et al., 2024). The focus here is on the agent's self-awareness in dynamic situations, rather than on static factual knowledge. Specifically, we define three types of situations based on agents' ability: - **Fast thinking**: The agent is able to directly provide the correct action with little thinking. - **Slow thinking**: The agent is able to provide the correct action but requires multiple steps of thinking and reflection. - **Knowledgeable thinking**: The agent is unable to provide the correct action and needs to rely on external knowledge for thinking. We go beyond the paradigm of fast or slow thinking (Yu et al., 2024; Saha et al., 2024; Chen et al., 2025; Li et al., 2025) and further introduce external knowledge into the thinking system of LLMs, striving to enhance the knowledgeable self-awareness ability of language agents. ## 3 Method ### 3.1 Knowledge System Construction Given that our emphasis is on knowledgeable self-awareness rather than the construction of a knowledge system, we draw upon and polish up a simple yet effective knowledge collection method outlined in Chen et al. (2024) to minimize costs in this process. The formation of the knowledge base is offline and lightweight, relying on an extremely minimal number of trajectories to be completed. A detailed knowledge system construction process can be found in Appendix A. We denote the final knowledge system as $S : (K, R)$ , where $K = \{k_1, k_2, ..., k_{N_{max}}\}$ is the knowledge base and $R$ is the knowledge selection module that can select the required knowledge based on the agent's historical trajectory $h_t$ . Please note that here our concept of "knowledge" can encompass various forms of knowledge sources, such as symbolic knowledge, parameterized knowledge, knowledge obtained through web searches, etc. Additionally, we have conducted analytical experiments on different retrievers in the Appendix I. ### 3.2 Situation Judgement Criterion Based on Equation 1 and our definition of three situations in 2, we classify the agent's situations into three types. Assuming the given history is denoted as $h_t$ , the gold next action is described as $a_{t+1}$ , and the next action predicted directly by the agent is represented as $a^p_{t+1}$ . We allow the agent to rethink when the predicted action is incorrect, resulting in a revised action denoted as $a^r_{t+1} = \text{rethink}(h_t, a^p_{t+1})$ . We then determine the agent's situation according to the following criteria $C$ : i) **Fast Thinking**: $a^p_{t+1} = a_{t+1}$ . The agent can directly generate the correct action. ii) **Slow Thinking**: $a^p_{t+1} \neq a_{t+1}, a^r_{t+1} = a_{t+1}$ . The agent can generate the correct action but needs rethinking. iii) **Knowledgeable Thinking**: $a^p_{t+1}, a^r_{t+1} \neq a_{t+1}$ . The agent is unable to generate the correct action, so it needs knowledge. This criterion will guide us in building situation awareness data, enabling the agents to make autonomous judgments about situations themselves. The selective mechanism will largely reduce the training and inference cost of excessive reflection and knowledge. ### 3.3 Self-awareness Apply We design a data-driven method called KnowSelf to endow the agent with agentic knowledgeable self-awareness capabilities as shown in Figure 2. **Data Construction**. Given the history-action pair $(h_t, a_{t+1})$ and an untrained agent $\pi_\theta$ , we augment the original action based on the situation criterion $C$ to construct the supervised self-awareness data. If the agent determines a correct action $a^p_{t+1}$ (Fast Thinking), $y = a_{t+1}$ will be directly used as the output. If the agent provides an incorrect action $a^p_{t+1}$ in the first trial, it will be given a prompt to rethink $^2$ . The chain of thought during this rethinking process is denoted as $ret$ . If the determined action $a^r_{t+1}$ after rethinking is correct (Slow Thinking), the output at this point is: $$ y = [a^p_{t+1}, \text{Reflection} <r>ret</r>, a_{t+1}], $$ where $[]$ represents concat with $\backslash n$ , Reflection is a special token used to mark the situation of Slow Thinking, $<r>$ and $</r>$ are special tokens surrounding the $ret$ . However, if the reflected action $a^r_{t+1}$ is incorrect, we introduce knowledge (Knowledgeable Thinking). We use the selection model $R$ to choose the most appropriate piece of knowledge $^3$ $know$ from the knowledge base $K$ and then the output at this situation is: $$ y = [ \text{Knowledge} <k>know</k>, a_{t+1}], $$ where Knowledge is the situational special token, $<k>$ and $</k>$ are special tokens surrounding the knowledge. After traversing all input-output pairs, we obtain the self-awareness training data $D_{self}$ . **Self-awareness Learning**. We apply a two-stage training process to teach the naive agent on our curated agentic knowledgeable awareness dataset $D_{self}$ . First, we train with the autoregressive loss to obtain the reference agent $\pi_{ref}$ : $$ L_{SFT} = -E_{(h_t,y)\sim D_{self}} \log \pi_\theta(y|h_t). $$ Then we enable the reference agent to explore on $D_{self}$ and collect the predicted $y^p$ with wrong actions as negative samples to construct a pair-wise awareness dataset $D_{pair}$ . In the second stage, we additionally introduce an offline DPO objective to further boost the self-awareness performance: $$ L_{DPO} = -E_{(h_t,y,y^p)\sim D_{pair}} \left[ \log \sigma \left( \beta \log \frac{\pi_\theta(y|h_t)}{\pi_{ref}(y|h_t)} - \beta \log \frac{\pi_\theta(y^p|h_t)}{\pi_{ref}(y^p|h_t)} \right) \right]. $$ Due to the narrow space of correct actions, following Pang et al. (2024), we re-introduce the SFT loss and normalize it by the output length in the second stage to stabilize the training process: $$ L_{NLL} = -E_{(h_t,y,y^p)\sim D_{pair}} \frac{\log \pi_\theta(y|h_t)}{|y|}, $$ resulting in the final loss for this stage: $$ L_{RPO} = L_{DPO} + \alpha L_{NLL}, $$ where $\alpha$ is a hyperparameter to balance the two loss terms. During training, we expand the vocabulary of models to adapt to the added special tokens. We analyze the impact of different training strategies for self-awareness performance in Appendix H. **Self-awareness Inference**. During the inference process, if the agent stops outputting after the first trial, we directly place the predicted action in the history $h_t$ for the next-step decision. If the agent generates Reflection after the first action, we allow it to continue the reflective process and place the reflected action into $h_t$ . If the agent directly generates Knowledge, we use $R$ to choose a piece of knowledge from $K$ . We append the selected knowledge to the context to allow the agent to continue this step, and then place the generated action into the history for the next decision. A running example of KnowSelf inference can be seen in Figure 2. ## 4 Experiments ### 4.1 Experimental Settings **Datasets and Metrics**. We evaluate KnowSelf on two real-world simulated planning datasets: ALFWorld (Shridhar et al., 2021) and WebShop (Yao et al., 2022). ALFWorld is a household dataset requiring the agent to navigate through the room and manipulate objects. The reward of ALFWorld is binary 0 or 1, indicating whether the agent has completed the task or not. WebShop is an online shopping dataset in a website environment. It provides dense final rewards from 0 to 1 to measure the completion level of the task. So for all the datasets, we apply Average Reward as the final metrics. We also include Know%=0% Our gold training trajectories are sourced from AgentBank (Song et al., 2024a). For more details of each dataset, please refer to Appendix E. **Models and Baselines**. We evaluate KnowSelf on two open-source models with different scales: 1) Gemma-2B (Rivière et al., 2024), the gemma-2-2b-it version; 2) Llama-8B (Dubey et al., 2024), the Llama-3.1-8B-Instruct version. To demonstrate validity, we compare KnowSelf with one general agent planning methods: REACT (Yao et al., 2023); two agent planning methods with trial-and-error: Reflexion (Shinn et al., 2023) and ETO (Song et al., 2024b); three knowledge-augmented methods: ExpeL (Zhao et al., 2024a), KnowAgent (Zhu et al., 2024), WKM (Qiao et al., 2024b). We also include GPT-4o (gpt-4o-2024-08-06) (Hurst et al., 2024) as a strong upper-bound baseline. We further introduce Know% to represent the ratio of actions enhanced with knowledge to all actions. Note that all the prompt-based baselines are tested with two-shot examples. Please refer to Appendix F for more baselines and re-producing details. **Training and Inference Details**. For the first training stage, we apply a learning rate of 2e-5 and a batch size of 8. For the second training stage, the learning rate is set to 5e-7 and the batch size is 3. The $\beta$ in DPO loss is set to 0.5 and the balanced factor $\alpha$ is set to 1. We train 3 epochs during the first stage and 1 epoch for the second stage. For all the inferences, we fix the temperature at 0. All our experiments are conducted on 8 NVIDIA A800 80G GPUs. More details can be seen in Appendix G. ### 4.2 Main Results **Comparison with baselines w/o knowledge**. Table 2&3 show the comparison between our method and baselines without knowledge (Know%=0%). KnowSelf consistently demonstrates superiority over baselines without knowledge on both Llama-8B and Gemma-2B. The performance of Gemma-2B even surpasses that of GPT-4o's REACT. Furthermore, our Llama-8B model performs comparably to GPT-4o's Reflexion, with the latter allowing the model to attempt a task up to 5 times until successful which is essentially a performance akin to hit@5. These emphasize the importance of knowledge in agent planning. **Comparison with baselines w/ knowledge**. We also contrast with knowledge-enhanced baselines to illustrate the advantages of knowledgeable self-awareness. From Table 2&3, it can be observed that KnowSelf surpasses all 100% knowledge baselines with a minimal amount of knowledge.