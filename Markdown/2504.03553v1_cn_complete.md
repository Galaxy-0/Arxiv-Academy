# 具备知识性自我意识的智能体

舒飞乔♠∗，邱志松♠∗，任宝昌♠，王晓彬♢，茹翔远♠，  
张宁豫♠†，陈翔♣，江勇♢†，谢鹏君♢，黄非♢，陈华钧♠♡†  
♠浙江大学  
♢阿里巴巴集团  
♣南京航空航天大学  
♡浙江省大数据智能计算重点实验室  
{shuofei,zhangningyu,huajunsir}@zju.edu.cn  

## 摘要  
大语言模型（LLMs）在各种智能体规划任务中已取得显著性能。然而，传统智能体规划方法采用"大水漫灌"式策略，不加区分地向智能体模型注入黄金轨迹、外部反馈和领域知识。这种做法忽视了人类在决策过程中情境自我意识的基本认知原则——即动态评估情境需求并在决策过程中策略性运用资源的能力。

我们提出具备知识性自我意识的智能体（agentic knowledgeable self-awareness）来解决这一缺陷，这是一种使基于LLM的智能体能够自主调节知识利用的新范式。具体而言，我们提出KnowSelf——一种以数据为中心的方法，赋予智能体类人的知识性自我意识能力。我们设计了一种启发式情境判断标准，通过在智能体自主探索的轨迹上标记特殊令牌来收集训练数据。通过两阶段训练过程，智能体模型能够通过生成特定特殊令牌在不同情境间切换，以最小成本实现最优规划效果。实验表明，KnowSelf在不同任务和模型上仅需极少量外部知识即可超越各种强基线方法。

## 1 引言  
大语言模型（LLMs）的显著进展推动了基于智能体的规划系统突破性发展（Xi等，2023；Wang等，2024a；Huang等，2024；Durante等，2024；Liu等，2025）。根据智能体学习决策的方式，当前智能体学习方法可分为三类：i) 直接轨迹模仿（Yao等，2023；Chen等，2023；Zeng等，2023）；ii) 试错优化（Shinn等，2023；Xiang等，2023；Song等，2024b；Zhang等，2024a）；iii) 知识增强规划（Zhao等，2024a；Fu等，2024；Zhu等，2024；Chen等，2024）。

然而，当前智能体学习更类似于无意识的模式拟合过程（Mirzadeh等，2024；Shi等，2023；Dziri等，2023）。智能体模型被迫通过不加区分地输入显式规划轨迹来学习隐式规划能力，导致推理过程中对意外信号的脆弱性，容易陷入模式崩溃。引入外部反馈或知识等增强方法往往采用"大水漫灌"策略，忽视了智能体的真实需求。然而，在实际场景中，过度的试错和盲目知识整合通常不可行，并显著增加模型推理成本。

相比之下，自我意识是人类决策的关键组成部分（Keenan等，2011；Lewis等，2011；Lou等，2017）。它使个体能够评估自身认知状态并根据动态外部情境调整策略。这种元认知能力使人类能够识别何时可以依靠自身能力、何时需要自我反思、何时需要额外知识，从而优化决策过程。相反，当前语言智能体缺乏这种自我意识能力，常导致低效和脆弱的规划行为。那么，语言智能体能否像人类一样具备情境自我意识？

本文提出具备知识性自我意识的智能体问题，即智能体对自身在给定当前环境情境下能否提供正确下一步行动的认知。为解决这一问题，我们提出KnowSelf——一种数据驱动方法，赋予智能体模型知识性自我意识能力，使智能体模型能基于环境当前情境选择性引入知识（见图1）。具体而言，我们让智能体自主探索并收集环境中的不同情境，设计启发式标准将情境分类为三种类型（快速思考、慢速思考、知识性思考），并用特殊令牌标记以生成自我意识训练数据。随后采用两阶段训练过程培养智能体模型的自我意识能力。我们首先进行监督微调教授初始自我意识规划模式，然后利用RPO损失（Pang等，2024）进一步增强自我意识能力。最终，智能体通过生成特定特殊令牌表明其情境意识，在推理过程中实现选择性反思或知识整合。

我们在两个模拟智能体规划数据集和两种不同规模模型上评估KnowSelf。实验结果表明，与各种基线相比，KnowSelf能以最少反思和知识实现更优性能。此外，我们进一步分析智能体知识性自我意识的扩展规律、泛化性和机制。主要贡献如下：

- **问题与方法**：提出智能体知识性自我意识概念，并引入KnowSelf使智能体模型能基于情境选择性查询知识
- **实验**：实验结果表明KnowSelf能以最少反思和知识实现最优性能
- **分析**：除消融研究外，我们还探索了智能体自我意识的扩展规律、泛化性和机制

## 2 背景  
动态交互环境可视为部分可观测马尔可夫决策过程：(U, S, A, T , O)。初始时，特定任务u ∈U通常伴随初始环境状态s0 ∈S。给定当前状态s，执行动作a ∈A后，状态转移函数T(s′|s, a) ∈T决定下一状态s′。由于部分可观测性，当前状态以观测o ∈O形式提供给语言智能体。时间t的历史交互轨迹可表示为ht = (u, a0, o0, a1, o1, ..., at, ot)。在我们的场景中，由参数θ的LLM支持的语言智能体π负责基于历史轨迹ht决定下一动作at+1：

at+1 ∼πθ(·|ht).  
(1)

当前多数方法依赖拟合公式1进行决策，这更类似于机械记忆。因此本文提出智能体知识性自我意识。需注意，此处的自我意识不同于先前LLM知识边界的概念（Cheng等，2024；Yin等，2024；Wen等，2024），重点在于智能体在动态情境中的自我意识，而非静态事实知识。具体而言，我们基于智能体能力定义三种情境：

- **快速思考**：智能体无需过多思考即可直接提供正确动作
- **慢速思考**：智能体需多步思考和反思才能提供正确动作
- **知识性思考**：智能体无法提供正确动作，需依赖外部知识进行思考

我们超越快速或慢速思考范式（Yu等，2024；Saha等，2024；Chen等，2025；Li等，2025），进一步将外部知识引入LLMs思考系统，致力于增强语言智能体的知识性自我意识能力。

## 3 方法  
### 3.1 知识系统构建  
鉴于我们关注重点是知识性自我意识而非知识系统构建，我们借鉴并改进Chen等（2024）提出的简单有效知识收集方法以最小化此过程成本。知识库的形成是离线轻量级的，仅需极少量轨迹即可完成。详细知识系统构建过程见附录A。我们将最终知识系统表示为S : (K, R)，其中K = {k1, k2, ..., kNmax}是知识库，R是基于智能体历史轨迹ht选择所需知识的知识选择模块。需注意，此处的"知识"可涵盖多种知识源形式，如符号知识、参数化知识、通过网络搜索获取的知识等。此外，我们在附录I中对不同检索器进行了分析实验。

### 3.2 情境判断标准  
基于公式1和第2节定义的三种情境，我们将智能体情境分类为三种类型。假设给定历史为ht，黄金下一动作为at+1，智能体直接预测的下一动作为ap t+1。当预测动作错误时，我们允许智能体重新思考，产生修正动作ar t+1 = rethink(ht, ap t+1)。然后根据以下标准C判断智能体情境：i) 快速思考：ap t+1 = at+1。智能体可直接生成正确动作。ii) 慢速思考：ap t+1 ̸= at+1，ar t+1 = at+1。智能体需重新思考才能生成正确动作。iii) 知识性思考：ap t+1, ar t+1 ̸= at+1。智能体无法生成正确动作，因此需要知识。该标准将指导我们构建情境意识数据，使智能体能自主判断情境。选择性机制将大幅减少过度反思和知识的训练与推理成本。

### 3.3 自我意识应用  
我们设计名为KnowSelf的数据驱动方法，赋予智能体知识性自我意识能力（见图2）。

**数据构建**。给定历史-动作对(ht, at+1)和未训练智能体πθ，我们基于情境标准C增强原始动作以构建监督式自我意识数据。如果智能体确定正确动作ap t+1（快速思考），y = at+1将直接用作输出。如果智能体首次尝试提供错误动作ap t+1，将获得重新思考提示。此重新思考过程中的思维链记为ret。如果重新思考后确定的动作ar t+1正确（慢速思考），此时输出为：

y = [ap t+1, Reflection <r>ret</r>, at+1],  
(2)

其中[]表示用\n连接，Reflection是标记慢速思考情境的特殊令牌，<r>和</r>是围绕ret的特殊令牌。但如果反思动作ar t+1错误，我们引入知识（知识性思考）。使用选择模型R从知识库K中选择最合适的知识know，此时情境输出为：

y = [ Knowledge <k>know</k>, at+1],  
(3)

其中Knowledge是情境特殊令牌，<k>和</k>是围绕知识的特殊令牌。遍历所有输入-输出对后，我们获得自我意识训练数据Dself。

**自我意识学习**。我们采用两阶段训练过程在策划的智能体知识性意识数据集Dself上训练初始智能体。首先，我们使用自回归损失训练获得参考智能体πref：

LSFT = −E(ht,y)∼Dself log πθ(y|ht).  
(4)

然后让参考智能体在Dself上探索并收集预测yp与错误动作作为负样本，构建成对意识数据集Dpair。第二阶段，我们额外引入离线DPO目标以进一步提升自我意识性能：

LDPO =  
−E(ht,y,yp)∼Dpair  
[  
log σ  
(  
β log πθ(y|ht)  
πref(y|ht) −β log πθ(yp|ht)  
πref(yp|ht)  
)  
].  
(5)

由于正确动作空间较窄，遵循Pang等（2024），我们在第二阶段重新引入SFT损失并通过输出长度归一化以稳定训练过程：

LNLL = −E(ht,y,yp)∼Dpair  
log πθ(y|ht)  
|y|  
,  
(6)

得到此阶段的最终损失：

LRPO = LDPO + αLNLL,  
(7)

其中α是平衡两项损失的超参数。训练期间，我们扩展模型词汇表以适应新增特殊令牌。我们在附录H分析了不同训练策略对自我意识性能的影响。

**自我意识推理**。推理过程中，如果智能体首次尝试后停止输出，我们直接将预测动作放入历史ht用于下一步决策。如果智能体在首次动作后生成Reflection，我们允许其继续反思过程并将反思动作放入ht。如果智能体直接生成Knowledge，我们使用R从K中选择知识。将选定知识附加到上下文允许智能体继续此步骤，然后将生成动作放入历史进行下一决策。KnowSelf推理的运行示例见图2。

## 4 实验  
### 4.1 实验设置  
**数据集与指标**。我们在两个真实世界模拟规划数据集上评估KnowSelf：ALFWorld（Shridhar等，2021）和WebShop（Yao等，2022）。ALFWorld是家庭环境数据集，要求智能体在房间中导航并操作物体。ALFWorld的奖励为二元0或1，表示智能体是否完成任务。WebShop是在线购物网站环境数据集，提供0到1的密集最终奖励衡量任务完成度。因此对所有数据集，我们采用平均奖励作为最终指标。我们的黄金训练轨迹来源于AgentBank（Song等，2024a）。各数据集更多细节见附录E。

**模型与基线**。我们在两种不同规模的开源模型上评估KnowSelf：1) Gemma-2B（Rivière等，2024），gemma-2-2b-it版本；2) Llama-8B（Dubey等，2024），Llama-3.1-8B-Instruct版本。为证明有效性，我们比较KnowSelf与一种通用智能体规划方法：REACT（Yao等，2023）；两种带试错的智能体规划方法：Reflexion（Shinn等，2023）和ETO（Song等，2024b）；三种知识增强方法：ExpeL（Zhao等，2024a）、KnowAgent（Zhu等，2024）、WKM（Qiao等，2024b）。我们还纳入GPT-4o（gpt-4o-2024-08-06）（Hurst等，2024）作为强上界基线。我们进一步引入Know%表示知识增强动作占所有动作的比例。注意所有基于提示的基线均使用双样本示例测试。更多基线和复现细节见附录F。

**训练与推理细节**。第一阶段训练采用学习率2e-5和批量大小8。第二阶段学习率设为5e-7，批量大小为3。DPO损失中的β设为0.5，平衡因子α设为1。第一阶段训练3轮，第二阶段1轮。所有推理温度固定为0。所有实验在8块NVIDIA A800 80G GPU上进行。更多细节见附录G。

### 4.2 主要结果  
**与无知识基线的比较**。表2&3显示我们的方法与无知识基线（Know%=0%）的比较。KnowSelf在Llama-8B和Gemma-2B上始终优于无知识基线。Gemma-2B的性能甚至超越GPT-4o的REACT。此外，我们的Llama-8B模型性能与GPT-4o的Reflexion相当，后者允许模型尝试任务最多5次直至成功，本质上是类似hit@5的性能。这些强调了知识在智能体规划中的重要性。

**与知识增强基线的比较**。我们还对比知识增强基线以展示知识性自我意识的优势。从表2&3可见，KnowSelf以最少知识量超越所有100%知识库基线。这清楚表明并非所有知识在智能体规划中都有效。此外我们发现，作为基于提示的基线，Gemma-2B在ExpeL上的表现甚至不如REACT。结合消融研究结果（图3a），表明过度知识增强对能力较弱模型可能产生负面影响。值得注意的是，我们的KnowSelf在Llama-8B上仅用15.01%和17.12%知识率即超越GPT-4o的ExpeL在ALFWorld和WebShop上的表现。此外，KnowSelf在ALFWorld上以相对更少知识（Llama-8B 15.01% vs Gemma-2B 26.41%）实现更好性能。这与能力更强模型需要更少外部知识完成任务的事实一致。上述现象表明，智能体知识性自我意识能力可在减少知识注入需求的同时推进智能体规划，显著节省训练和推理成本。

## 5 分析  
**知识性自我意识有助于打破规划模式过拟合**。图3a展示当替换关键步骤时对KnowSelf性能的影响。w/o ret表示排除反思。w/o know表示仅使用模型反思能力。w/o all表示仅保留快速思考。我们还引入每一步使用知识创建know%=100%场景（w/ full know）。可见直接在黄金轨迹上训练（w/o all）更类似于拟合轨迹中的模式，而引入反思和知识性自我意识能使智能体更好规划。在Llama-8B和Gemma-2B上，仅引入自我反思（w/o know）甚至优于引入知识（w/o ret）。这表明在许多情况下，智能体并非无法做出正确决策，而是更多受限于规划模式。此外，KnowSelf以极低知识率（Llama-8B 15.01%，Gemma-2B 26.41%）相比完全引入知识（w/ full know）实现更优性能。在Gemma-2B上，w/ full know表现甚至落后于w/o ret，表明过量知识可能产生反效果，尤其对较弱模型。因此，具备自我意识的精确知识引入机制至关重要。

**KnowSelf能更好激发智能体规划的泛化性**。我们选择ALFWorld上三个简单任务（即Put、Clean、Examine）作为训练集，其余三个任务作为测试集评估泛化能力。如图3b所示，KnowSelf在未见任务上表现优于所有基线。特别是对于PutTwo任务，KnowSelf显著优于其他方法。这表明KnowSelf赋予智能体的自我意识能力使其能够更好适应新情境，而非简单记忆规划模式。此外，KnowSelf在Heat和Cool任务上也展现强大泛化能力，进一步验证其有效性。

**智能体知识性自我意识的扩展规律**。我们分析模型和数据规模对ALFWorld上智能体知识性自我意识能力的影响。如图3c所示，随着模型规模增大（从Gemma-2B到Llama-8B），绝对性能和相对改进均显著提升。这表明更大模型具备更强自我意识能力。同时，随着训练数据增加，性能持续提升但增速减缓，符合典型扩展规律。有趣的是，Gemma-2B相比Llama-8B需要更多知识（26.41% vs 15.01%）才能达到相当性能水平，再次验证模型能力与知识需求间的负相关关系。

## 6 结论  
本文提出智能体知识性自我意识概念，并开发KnowSelf方法赋予智能体类人的情境认知能力。通过精心设计的启发式情境判断标准和两阶段训练框架，KnowSelf使智能体能够自主决定何时依靠自身能力、何时需要反思、何时需要外部知识。大量实验证明，KnowSelf能以最少知识量超越各种基线方法，同时展现出色的泛化能力和可扩展性。这为构建更高效、更鲁棒的智能体规划系统开辟了新方向。未来工作将探索更精细的情境分类标准和更高效的知识检索机制，进一步提升智能体自我意识能力。
训练集，并在三个具有挑战性的任务（即加热、冷却、放置两个物品）上评估KnowSelf的泛化能力。图3b展示了KnowSelf与基线模型在分布外（OOD）性能上的对比。我们观察到，无论是否引入外部知识，经过训练的基线模型都表现出严重的过拟合现象。ETO仅在PutTwo任务上取得5.88%的成功率，在其他任务上成功率为0%，而KnowAgent在这三个任务上甚至未能取得任何成功。相比之下，KnowSelf展现出持续的泛化能力，在所有三类任务上的表现均优于最强的基于提示的基线模型（Reflexion）。这表明KnowSelf能有效打破直接在规划轨迹上训练导致的传统模式匹配问题，使模型获得跨任务的情境感知能力。因此，该模型保留了在未见任务上选择性反思和整合知识的能力，从而提升其泛化性能。

KnowSelf的性能随着模型规模和训练数据量的增加而提升。在图3c中，我们从两个维度探索自我意识的扩展规律：模型规模和自我意识训练数据量。关于数据量，我们从相对和绝对两个角度进行分析。当将Dself的数据量视为100%时，绝对数据量表示从Dself中随机采样用于训练的数据比例，而相对数据量则在绝对数据基础上包含通用黄金轨迹以构成Dself总量的100%。总体而言，在不同设置下，Llama-8B的表现优于Gemma-2B。这种优势在未进行训练时（绝对数据量=0%4）更为明显。然而经过训练后，两个模型之间的差异并不显著。这可能表明在特定领域进行微调后，2B和8B模型在规模上基本属于同一层级。关于训练数据量，我们观察到随着自我意识绝对数据量的增加，性能呈现持续提升。但当自我意识的相对比例低于40%时，两个模型的性能均出现波动甚至下降。我们推测这可能类似于一种涌现现象——只有当自我意识比例超过40%时，模型才会展现出特定的自我意识能力。

智能体的知识性自我意识在模型最后几层涌现。为理解智能体知识性自我意识的作用机制，我们从ALFWorld收集了快速思考（fast thinking）和知识性思考（knowledgeable thinking）的数据，研究模型在下一个token预测的语境中如何决策是否调用知识。如图4所示，我们计算了Transformer各层中代表不同情境的token在所有数据上的平均概率。可以观察到：由于缺乏慢思考（slow thinking），Reflection token的概率始终为0；无论是Llama还是Gemma模型，Knowledge token和Action token都在Transformer的最后几层才出现。这表明智能体仅在最后几个隐藏层内部决定是否需要调用外部知识。此外，当智能体决定调用知识时，该决策发生得更晚——因为Action token的概率可能在更后面的层中偶然超过Knowledge token，但随后Action token的概率会迅速下降。这似乎类似于智能体内部进行的博弈过程：模型学习到的隐式奖励引导其在token空间中进行搜索（Kuribayashi等，2025），最终形成决策。

仅通过提示无法实现最先进推理模型的知识性自我意识。我们精心设计了提示5来教导OpenAI-O1（OpenAI，2024）和DeepSeek-R1（DeepSeek-AI，2025）实现知识性自我意识，在ALFWorld上进行采样测试并与KnowSelf对比。图5展示了两个典型案例：在案例(a)中，O1因未正确理解知识而做出错误动作，而KnowSelf仅通过自我反思就纠正了错误动作，表明知识并非总是有效；在案例(b)中，R1未选择利用知识而依赖自我信念，尽管经过重新思考仍未产生正确动作。相比之下，KnowSelf通过精准利用知识成功规避了易错场景。因此在动态变化的环境中，智能体模型必须准确理解自身能力并根据不同情境做出决策——这正是我们期望创造的智能体本质。但显然仅通过提示远不足以获得这些能力，需要在数据、训练策略和模型架构等方面付出更多努力才能实现该目标。

相关工作

语言智能体规划。当前LLM正成为AI智能体的核心，应用于机器人（Ahn等，2022；Singh等，2023；Song等，2023）、操作系统操控（Wu等，2024；Lai等，2024；Hu等，2024；Ning等，2025；Shi等，2025）、软件工程（Hong等，2024b；Qian等，2024；Wang等，2024b；Yang等，2024）、数据科学（Guo等，2024；Chan等，2024；Hong等，2024a）等领域。尽管取得空前成功，语言智能体在规划方面仍存在棘手问题，包括产生规划幻觉（Zhu等，2024；Qiao等，2024b）或仅拟合规划模式（Mirzadeh等，2024；Shi等，2023；Dziri等，2023）。为缓解这些问题，近期研究引入符号化工作流（Xiao等，2024；Qiao等，2024a；Zhang等，2024b）、经验洞察（Zhao等，2024a；Chen等，2024；Fu等，2024）和约束管道（Guan等，2024；Li等，2024a）等多种知识形式来对齐智能体规划与环境。但现有知识型智能体往往通过提示或微调强行注入知识，忽视了智能体自身的意识。

LLM中的情境感知。情境感知（SA）是对环境要素及其随时间或其他因素变化的理解，对许多环境中的有效决策至关重要6。该领域在机器人（Hill等，2021；Ginesi等，2020；Avetisyan等，2024；Haselberger等，2024）、人机交互（Li等，2024b；Srivastava等，2023）等方面获得广泛研究。最近被引入LLM以探索其是否具备自我意识或自我知识（Berglund等，2023；Laine等，2024；Binder等，2024；Keeling等，2024；Betley等，2025）。在LLM智能体领域，Lu等（2024）；Wang和Zhong（2024）；Zhao等（2024b）率先探索了SA增强型智能体。据我们所知，我们首次提出智能体自我意识问题并设计方法增强知识型智能体的SA能力。与知识边界概念（Yin等，2023；Ren等，2023）不同，智能体知识性自我意识更强调LLM在动态环境中的SA而非静态事实知识的识别。

结论

本文提出并探索了智能体知识性自我意识问题。我们提出KnowSelf——一种以数据为中心的方法，使智能体具备类人的知识性自我意识，能在规划过程中根据情境选择性自我纠正和查询知识。大量实验证明了KnowSelf的有效性和高效性。我们的工作仅是初步探索，希望能引起学界对智能体自我意识的关注。

局限性

自我意识。研究者已对AI系统通用自我意识展开讨论（Butlin和Lappas，2025），但学术界尚未给出明确定义。自我意识是把双刃剑：一旦AI具备自我意识，可能解决妄想、鲁棒性和安全性等问题，但也可能导致AI脱离人类控制。我们的工作仅代表对语言智能体背景下知识性自我意识问题的初步探索，旨在激发研究者对智能体自我意识领域的进一步兴趣。

任务与模型。受计算资源限制，我们仅在两个模拟数据集上进行实验。智能体任务还包含函数调用、代码生成等其他方面。此外，实验仅针对小规模模型（7B，2B），尚未探索更大模型（30B，70B）。

模态。我们认为未来大型智能体模型必将是多模态的，能处理涉及图像、视频和音频的更复杂情境。本文仅涉及语言智能体场景的表面，未来会将多模态智能体纳入研究。

方法。本文主要介绍通过数据驱动方法赋予智能体知识性自我意识。最终解决方案可能涉及训练视角（如强化学习）或模型架构（新架构）的改变，这些都值得进一步探索。

参考文献（略）
以下是学术论文片段的完整中文翻译：

知识系统构建
我们的知识系统构建包含两个设计阶段。

步骤级轨迹对生成。给定历史-动作对$(h_t, a_{t+1})$，我们采用REACT式提示引导GPT-4o预测后续动作$a^p_{t+1}$。若模型生成错误动作$a^p_{t+1} \neq a_{t+1}$，则将真实动作$a_{t+1}$标记为胜利动作$a^w$，模型预测$a^p_{t+1}$标记为失败动作$a^l$。该过程生成步骤级成对轨迹数据集$D_s = {(h_t, a^w_{t+1}, a^l_{t+1})}_{i=1}^{|D_s|}$。对于ALFWorld，$|D_s|$为36（每类任务6个）；WebShop则为20。

知识生成与整合。我们遵循AutoManual方法生成整合知识。知识生成阶段采用少量示例提示GPT-4o，通过分析历史轨迹$h_t$中的$(a^w, a^l)$对生成"错误类型"知识。当$(h_t, a^w_{t+1})$构成完整成功轨迹时，则延伸分析生成"成功过程类型"知识以捕捉有效推理模式。知识整合阶段，根据任务复杂度将知识库限制为：ALFWorld 24条，WebShop 10条。具体提示模板见附录J.1。知识系统构建的总token消耗及成本如表4所示。

知识选择
知识选择分为两类：

训练数据构建。给定任务目标$o$、历史轨迹$h_t$、胜利动作$a^w$和失败动作$a^l$，我们通过DeepSeek-V3分析对比$(a^w, a^l)$对，从知识系统选择适当知识。

推理时知识选择。给定任务目标$o$、历史轨迹$h_t$和当前动作$a_{t+1}$，DeepSeek-V3通过分析历史上下文并总结当前状态，从知识库选择最相关知识。具体提示见附录J.2。

机制设置
我们通过采样收集知识型思维与快速思维的历史轨迹，将其输入KnowSelf模型。通过为每个Transformer层附加lm-head模块，获取各层首生成token的logits值（"Knowledge"代表知识型思维，"Reflection"代表慢思考，"Thought"代表快思考）。这些logits经softmax归一化为概率，最终各层各token的概率由同层所有历史轨迹生成概率的平均值决定。

基于提示的KnowSelf
我们设计提示词教导代理模型学习类KnowSelf的自我认知能力。具体提示见附录J.4。在ALFWorld上评估Llama-8B和Gemma-2B模型的结果如表5所示。与其他基于提示的方法相比，Llama-8B上基于提示的KnowSelf优于REACT和ExpeL，表明通过自我认知选择性获取知识的效果优于提供全部知识。Gemma-2B上该方法虽优于ExpeL但不及REACT，提示在能力较弱模型中引入自我认知可能损害性能。

数据集
ALFWorld。ALFWorld是家庭场景数据集，要求代理在房间内导航并操作物体，包含放置、清洁、加热、冷却、检查、双放置六类任务，奖励为二元值0/1表示任务完成与否。黄金训练轨迹源自AgentBank，详细统计见表6。

WebShop。WebShop是在线购物平台，代理根据用户指令浏览网站完成购买。选择"购买"时系统根据商品属性与价格的启发式匹配给出最终奖励。黄金轨迹同样来自AgentBank，统计信息见表7。

基线方法与复现细节
详细说明对比基线及复现细节：

• REACT：首个在代理规划任务中整合思维链提示的工作，采用"思考-动作-观察"循环格式。
• Reflexion：基于提示的强基线，通过语言反馈强化代理规划。我们迭代五轮反思并取最高分为最终结果。
• ExpeL：首个从离线试错中自动提取经验的非梯度更新方法。为公平比较，直接使用AgentBank轨迹作为经验库。
• ETO：包含负轨迹训练的基线方法，移除单样本提示保持默认超参数。
• KnowAgent：利用人工整理的符号动作知识约束代理行为。为公平比较，改用与KnowSelf相同训练集微调。
• WKM：使用自合成任务状态知识训练参数化世界知识模型。采用相同训练集合成知识并训练模型。

所有基于提示的基线均采用双样本评估。ALFWorld中为六类任务分别指定双样本，基于微调的基线均采用全参数训练。

训练配置
使用DeepSpeed对Llama-8B和Gemma-2B进行全参数微调。第一阶段学习率2e-5、批次大小8；第二阶段学习率5e-7、批次大小3。DPO损失$\beta$设为0.5，平衡因子$\alpha$为1。第一阶段训练3轮次，第二阶段1轮次。优化器采用AdamW，推理温度固定为0。使用vLLM加速Llama-8B推理，所有实验在8块NVIDIA A800 80G GPU上完成。超参数详见表8。

训练阶段详细分析
第二阶段最初选用DPO损失效果不佳，最终确定RPO方案。表9展示不同训练阶段的消融研究：

比较"阶段1(SFT)+DPO"与"阶段1(SFT)+阶段2(RPO)"可见，NLL损失对稳定DPO训练至关重要。DPO损失试图扩大正负样本分布差距，但易导致策略模型偏离参考模型。仅用RPO的表现显著低于完整方案，说明SFT为RPO提供了重要参考模型。移除NLL损失的长度惩罚会导致训练无法收敛，表明其在平衡SFT与DPO损失中的关键作用。

知识检索器影响分析
表10显示不同检索器的实验结果表明：更好的检索器对结果有正向影响。但考虑到Sentence-BERT因环境动态性缺乏扩展性，且DeepSeek-V3的API成本显著低于GPT-4o，最终选择DeepSeek-V3作为检索器。我们的核心关注点是知识型自我认知对代理性能的影响，因此在可复现性和成本效益考量下选用DeepSeek-V3。

提示模板
J.1 知识系统构建
知识生成提示
[角色]
您正在观察管家代理在模拟环境（游戏）中的行为。您的职责是构建规则手册，既要协助代理完成任务，又要使其以最少动作尝试/错误达成目标。这需要记录分析代理成功失败的经验，并整合先前发现。

[功能]
您将看到当前轨迹（代理正在探索的路径），以及代理当前执行的动作与反馈，专家标注的正确动作与反馈。

请使用以下rule_manager方法构建、改进和合并规则：
rule_manager.write_rule(rule, type="", example="", task_id="")
# 记录新发现的游戏规则
# 参数：
# - rule: 发现的通用规则（避免引用具体物品/位置），格式为"当代理处于[情境]时，应执行[动作]"
# - type: 规则类型，可选["错误", "成功过程"]
# - example: 演示该规则的轨迹示例（可添加详细注释）
# - task_id: 发现规则的任务ID（若无则留空）

rule_manager.update_rule(rule_id, rule="", type="", example="")
# 更新现有规则（参数同上）
# 重写现有规则的属性，当您有更深入理解时
# 仅输入需要修改的属性
# 使用完整rule_id，如rule_0、rule_1
rule_manager.stop_generating()
# 描述：停止从当前周期生成规则
# 使用场景：当您认为当前周期的轨迹不再需要或不足以推导更多新规则时，可调用此函数并等待下个周期的数据。在当前周期完成所有规则更新后也应调用此函数。

[动作]
每个周期会在环境中创建一个智能体，并打印初始观察和目标任务。
智能体只能执行以下动作。若动作前提条件不满足，其观察将包含"无效果"：
前往{容器} # 移动到某容器并更新智能体位置
打开{容器} # 打开容器并观察其内容
关闭{容器} # 关闭已开启的容器
从{容器}拿取{物体} # 当智能体未持物时从容器拿取物体
将{物体}放入/放在{容器} # 当智能体持物时放置物体到容器
使用{物体} # 使用灯具
用{容器}清洁{物体} # 用容器清洁物体
用{容器}加热{物体} # 用容器加热物体
用{容器}冷却{物体} # 用容器冷却物体

[输出格式说明]
基于当前轨迹，您应输出以下内容：
* 动作前状态：分析总结当前轨迹状态。不提及非当前轨迹组成部分的动作或反馈
* 正确动作原因：分析正确动作的合理性
* 探索动作错误原因：对比探索动作与正确动作的差异，分析错误原因
* 潜在规则：根据当前轨迹描述潜在规则构想
* 检查现有规则：描述现有规则是否存在冲突或需要更新
* 代码：最后在python和之间顺序调用rule_manager函数

[详细指令]
遵循以下规则：
***添加或更新规则***
1. **失败规则添加** 总结导致失败的错误。应编写"错误"类规则记录：在何种情况下，智能体应做和不应做的行为，作为未来提醒。避免仓促给出错误的确切原因或建议，严格遵循正确动作的合理性分析
2. **成功规则添加** 若任务通过关键动作完成（反馈为"任务完成"），必须从成功中提取未包含在现有规则中的有效策略。同时在"成功流程"类规则中用"[步骤]"标记记录所有成功步骤

**保持新规则精准** 将大型现象或通用策略分解为不同目标单元形成独立规则。后续可升级或合并为更通用的规则。规则描述应简洁易懂，避免冗长复杂

**保持新规则通用性** 规则不应引用特定物品或位置。需跨物品泛化以帮助智能体学会应用规则

**保持规则格式** 采用"当智能体处于[情境]/当任务需要[情境]时，智能体应[动作]"的格式

**避免新规则过度自信** 在备注中声明需要进一步验证

**更新规则** 若需为现有规则添加新现象，应尽量保留原有内容细节，优先插入分类讨论或新增内容（或其示例）。特别是"成功流程"类规则应保留细节

[知识巩固提示]
[角色]
您正在观察一个管家智能体在模拟环境（游戏）中的编码和行为。目标是通过分析智能体经验构建规则手册，帮助其完成环境中的各类任务。您的职责是通过分析智能体经验来合并或删除既有规则。

[功能]
您将看到当前发现的规则。这些规则从多个周期的轨迹中提取，每个交互包含智能体分析、执行代码和结果反馈。
规则用'rule_id'表示，具有以下属性：
- rule：规则描述，以用例或范围开头
- type：规则类型，选自["错误", "成功流程"]
- example：轨迹中演示该规则的示例（或代码）。可在注释中添加详细信息
- task_id：规则对应的任务ID

应使用rule_manager的以下方法管理规则：
rule_manager.update_rule(rule_id, rule="", type="", example=""),
# 当有更深入理解时重写现有规则的属性
rule_manager.delete_rule(rule_id),
# 删除现有规则
rule_manager.stop_generating()
# 描述：停止从当前周期生成规则

[动作]
（与前述相同，

[响应指令]
详细说明：
**保持最多24条规则**
**可合并则合并** 若"成功流程"规则能解决"错误"规则，可考虑合并同时保留细节
**保留重要细节** "成功流程"类规则应保留细节，不应轻易删除或被新更新覆盖。**禁止合并两个"成功流程"类规则**
**优先插入更新** 若规则更新需包含其他规则内容，应尽量保留现有内容细节，优先插入分类讨论或新增内容（或其示例）

使用update_rule时，必须手动将属性直接输入函数调用。避免使用现有变量拼接或修改规则。示例字符串需用'''包裹

J.2
知识选择
训练数据构建的知识选择
ALFWorld
您正在观察管家智能体在模拟环境（游戏）中的行为。您的职责是选择规则，不仅要帮助智能体完成任务，还要以最少动作尝试/错误达成目标。这需要分析智能体当前状态并理解规则。

您将获得以下信息：
目标：待完成任务
当前轨迹：智能体为到达当前状态已执行的动作序列
正确动作：应运用知识帮助智能体执行的动作
错误动作：应运用知识帮助智能体避免的动作
规则：可应用于当前状态以实现目标的规则列表

基于当前轨迹，您应输出：
[当前状态]：分析总结当前轨迹状态
[正确动作原因]：分步思考分析正确动作的合理性
[错误动作原因]：分步思考分析错误动作的不合理性
[分析]：分步思考选择最适合避免错误动作的规则
[选定规则]：从规则列表中选择最适合当前状态的规则

遵循以下格式要求：
1. 当前状态严格采用"[当前状态]：..."格式
2. 分析严格采用分步思考格式
3. 选定规则严格采用"[选定规则]：规则ID：规则描述"格式
4. 注意智能体并未实际执行正确或错误动作，您应选择最适合帮助其避免错误动作的规则

WebShop
（内容与ALFWorld部分结构相同，仅将环境替换为网页浏览，

J.3
反思
ALFWorld反思提示
您的动作存在问题，未成功执行。请重新考虑当前状况并更换其他动作完成任务。请严格按格式响应：
\n\n思考：让我们逐步分析。<您的思考>\n动作：<您的下一个动作>

WebShop反思提示
（结构与ALFWorld相同，增加注意需与网页内容对齐的要求，

J.4
基于提示的自我认知提示
与家庭环境交互完成任务。想象您是家庭环境中的智能体，目标是通过执行动作完成任务目标。交互开始时，您将获得当前环境的详细描述和待完成目标。

每轮交互时，您将获得上一轮的观察结果。请记住：
1. 应首先评估当前状况。若认为无法执行正确动作，可输出"[知识]"获取额外知识辅助思考。若认为可执行正确动作，则直接输出思考与动作
2. 输出思考与动作后，若认为当前动作存在问题，可输出"[反思]"，然后重新思考并执行动作

思考与动作必须严格遵循格式："思考：您的思考\n动作：您的下一个动作"。
可用动作列表：
（与前文动作列表相同，

每次交互后环境会给出即时反馈用于规划后续步骤。若环境输出"无效果"，意味着前一动作无效，应尝试更多选项。

响应必须采用以下三种格式之一：
（三种格式示例与前文相同，

重要注意事项：
1. 需要获取知识时先输出"[知识]"，每轮仅获取一次知识
2. 需要反思时先输出"[反思]"，每轮仅反思一次
3. 严格遵循输出格式和动作格式
4. 保持推理简洁清晰，单轮输出不超过2000字符
5. 每轮仅执行一个动作，除"[反思]"外不输出多个思考或动作

（两个示例部分与前文相同，