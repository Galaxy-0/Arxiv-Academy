
# 具身知识性自我认知  
乔硕飞♠∗，邱志松♠∗，任宝昌♠，王晓宾♢，茹翔远♠，  
张宁豫♠†，陈相元♣，蒋勇♢†，谢鹏军♢，黄非♢，陈华钧♠♡†  
♠浙江大学  
♢阿里巴巴集团  
♣南京航空航天大学  
♡浙江省大数据智能计算重点实验室  
{shuofei,zhangningyu,huajunsir}@zju.edu.cn  

## 摘要  
大语言模型(LLMs)在各种具身规划任务中展现出卓越性能。然而传统智能体规划方法采用"大水漫灌"式策略，不加区分地向模型注入黄金轨迹、外部反馈和领域知识。这种做法忽视了人类决策过程中情境自我认知的基本原理——即动态评估情境需求并策略性调配资源的能力。  

我们提出**具身知识性自我认知**来解决这一缺陷，这是一种使基于LLM的智能体能够自主调控知识利用的新范式。具体而言，我们提出KnowSelf——一种以数据为中心的方法，赋予智能体类人的知识性自我认知能力。通过设计启发式情境判断标准对智能体自主探索的轨迹进行特殊标记来收集训练数据，再经过两阶段训练使模型能通过生成特定标记在不同情境间切换，以最小成本实现最优规划效果。实验表明KnowSelf在最小化外部知识使用的前提下，能在不同任务和模型上超越多种强基线方法。

## 1 引言  
大语言模型的显著进展推动了基于智能体的规划系统突破(Xi等, 2023; Wang等, 2024a)。当前智能体学习方法可分为三类：i) 直接轨迹模仿(Yao等, 2023)；ii) 试错优化(Shinn等, 2023)；iii) 知识增强规划(Zhao等, 2024a)。  

然而现有方法更接近无意识的模式拟合过程(Mirzadeh等, 2024)，智能体被迫通过被动接受显式规划轨迹来学习隐式规划能力，导致推理过程对意外信号异常脆弱，容易陷入模式崩溃。引入外部反馈或知识的增强策略往往忽视智能体的真实需求，而过度的试错和盲目知识融合会显著增加推理成本。  

相比之下，自我认知是人类决策的关键组件(Keenan等, 2011)，使个体能评估认知状态并动态调整策略。这种元认知能力让人类知道何时依靠自身能力、何时需要反思或外部知识。本文提出**具身知识性自我认知**问题，即智能体对当前情境下能否提供正确行动的认知能力。  

## 2 背景  
动态交互环境可建模为部分可观测马尔可夫决策过程$(U, S, A, T , O)$。我们基于智能体能力定义三种情境：  
- **快速思考**：无需反思即可提供正确行动  
- **慢速思考**：需多步反思才能提供正确行动  
- **知识性思考**：需依赖外部知识才能提供正确行动  

## 3 方法  
### 3.1 知识系统构建  
采用轻量级知识收集方法构建知识系统$S : (K, R)$，其中$K = \{k_1, ..., k_{N_{max}}\}$为知识库，$R$为基于历史轨迹$h_t$的知识选择模块。  

### 3.2 情境判断标准  
给定历史$h_t$、黄金行动$a_{t+1}$和预测行动$a^p_{t+1}$，通过反思获得修正行动$a^r_{t+1}$后：  
1. 快速思考：$a^p_{t+1} = a_{t+1}$  
2. 慢速思考：$a^p_{t+1} \neq a_{t+1}$但$a^r_{t+1} = a_{t+1}$  
3. 知识性思考：$a^p_{t+1}, a^r_{t+1} \neq a_{t+1}$  

### 3.3 自我认知应用  
**数据构建**：根据情境标准$C$对原始行动进行增强：  
- 慢速思考输出：$y = [a^p_{t+1}, \text{Reflection}<r>ret</r>, a_{t+1}]$  
- 知识性思考输出：$y = [\text{Knowledge}<k>know</k>, a_{t+1}]$  

**两阶段训练**：  
1. 监督微调：$L_{SFT} = -\mathbb{E}_{(h_t,y)\sim D_{self}} \log \pi_\theta(y|h_t)$  
2. RPO优化：$L_{RPO} = L_{DPO} + \alpha L_{NLL}$  

## 4 实验  
### 4.1 实验设置  
在ALFWorld和WebShop数据集上评估，使用Llama-8B和Gemma-2B模型，比较REACT、Reflexion等基线方法。  

### 4.2 主要结果  
KnowSelf在Llama-8B上仅使用15.01%知识量即超越GPT-4o的ExpeL方法，Gemma-2B版本甚至超越GPT-4o的REACT。实验表明：  
- 知识性自我认知能有效打破规划模式过拟合  
- 具有优异的跨任务泛化能力  
- 性能随模型规模和数据量提升而增强  

## 5 分析  
**机制研究**：如图4所示，知识性自我认知的决策主要发生在Transformer的最后几层，呈现出类似博弈的搜索过程。  

**提示工程局限**：即使对GPT-4o等先进模型，仅通过提示也难以实现有效的知识性自我认知。  

## 数学公式示例  
状态转移函数：  
$$T(s'|s,a) \in T$$  

决策过程：  
$$a_{t+1} \sim \pi_\theta(\cdot|h_t)$$  

DPO损失函数：  
$$L_{DPO} = -\mathbb{E}_{(h_t,y,y^p)\sim D_{pair}} \left[ \log \sigma \left( \beta \log \frac{\pi_\theta(y|h_t)}{\pi_{ref}(y|h_t)} - \beta \log \frac{\pi_\theta(y^p|h_t)}{\pi_{ref}(y^p|h_t)} \right) \right]$$  

## 表格示例  
| 情境类型       | 触发条件                      | 输出格式                          |
|----------------|-----------------------------|-----------------------------------|
| 快速思考       | $a^p_{t+1} = a_{t+1}$       | $a_{t+1}$                        |
| 慢速思考       | $a^p_{t+1} \neq a_{t+1}$但反思成功 | $[a^p_{t+1}, \text{Reflection}<r>ret</r>, a_{t+1}]$ |
| 知识性思考     | 反思失败                    | $[\text{Knowledge}<k>know</k>, a_{t+1}]$ |
